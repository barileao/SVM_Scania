{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm_scania.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barileao/SVM_Scania/blob/master/svm_scania.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AnIlkd7dxY",
        "colab_type": "text"
      },
      "source": [
        "# Instalando packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpk8M8hHo5ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install pyod\n",
        "#!pip install impyute\n",
        "#pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt7IbQBD7qGv",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WOOD2BxHsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing all the necessary libraries\n",
        "from google.colab import files\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import SimpleImputer,MissingIndicator\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix,f1_score,log_loss\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "#from bayes_opt import BayesianOptimization\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsECpgVD7v_y",
        "colab_type": "text"
      },
      "source": [
        "# Acessar os dados no Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WMTkGuy31Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w5_L8iQqG-9",
        "colab_type": "text"
      },
      "source": [
        "# Upload dos dados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz1QLE2u7-PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filetrain = r'/drive/My Drive/IFES/TCC_Scania/Codes/aps_failure_training_set.csv'\n",
        "filetest = r'/drive/My Drive/IFES/TCC_Scania/Codes/aps_failure_test_set.csv'\n",
        "\n",
        "train = pd.read_csv(filetrain)\n",
        "print('Number of data points in the train dataset', train.shape[0])\n",
        "print('Number of features in the given train dataset', train.shape[1])\n",
        "test = pd.read_csv(filetest)\n",
        "print('Number of data points in the test dataset', test.shape[0])\n",
        "print('Number of features in the given test dataset', test.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAKQGUv1uo0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ----------------------Train--------------------------------\n",
        "\n",
        "# Replacing all the class labels as follows:\n",
        "\n",
        "# neg = 0\n",
        "# pos = 1\n",
        "class_labels_train = []\n",
        "for x in tqdm(train.index):        # tqdm = barra de progresso\n",
        "    if train['class'].loc[x] == 'neg':\n",
        "        class_labels_train.append(0)\n",
        "        \n",
        "    else:\n",
        "        class_labels_train.append(1)\n",
        "        \n",
        "\n",
        "train['class'] = class_labels_train\n",
        "\n",
        "# Distrubution of class labels in train dataset\n",
        "train['class'].value_counts()\n",
        "\n",
        "# ----------------------Test--------------------------------\n",
        "class_labels_test = []\n",
        "for x in tqdm(test.index):        # tqdm = barra de progresso\n",
        "    if test['class'].loc[x] == 'neg':\n",
        "        class_labels_test.append(0)\n",
        "        \n",
        "    else:\n",
        "        class_labels_test.append(1)\n",
        "        \n",
        "\n",
        "test['class'] = class_labels_test\n",
        "\n",
        "# Distrubution of class labels in train dataset\n",
        "test['class'].value_counts()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPn2ocekungQ",
        "colab_type": "text"
      },
      "source": [
        "# Histograma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMomDGqHyCnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Histogram plot of class labels of train dataset\n",
        "plt.hist(train['class'])\n",
        "plt.title(\"Histogram plot of class labels\")\n",
        "print(\"Percentage of negative class in the given dataset :\", (train[train['class']== 0].shape[0] / train.shape[0]) * 100)\n",
        "print(\"Percentage of positive class in the given dataset :\", (train[train['class']== 1].shape[0] / train.shape[0]) * 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPn8pYIkuCht",
        "colab_type": "text"
      },
      "source": [
        "# Remover e substituir NAN e NA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idr4ckL73Lpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ----------------------Train--------------------------------\n",
        "# Replacing the na and nan values with np.NaN\n",
        "train.replace(to_replace='nan', value = np.NaN,inplace = True)\n",
        "\n",
        "train.replace(to_replace = 'na', value = np.NaN, inplace = True)\n",
        "\n",
        "columns_names = train.columns\n",
        "\n",
        "train.describe(include='all')\n",
        "\n",
        "# Dropping all the columns where the number of missing values are more than 42K (>70%)\n",
        "#train.dropna(axis = 1, thresh=42000,inplace= True)\n",
        "#train.head()\n",
        "\n",
        "# ----------------------Test--------------------------------\n",
        "\n",
        "test.replace(to_replace='nan', value = np.NaN,inplace = True)\n",
        "\n",
        "test.replace(to_replace = 'na', value = np.NaN, inplace = True)\n",
        "\n",
        "test = test[train.columns]\n",
        "test.describe(include='all')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrQfga3B72HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL64ZFRNvSaG",
        "colab_type": "text"
      },
      "source": [
        "# Retirar os Outliers para o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArTM9UftvWd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coletando os outliers do train para 'trainol'\n",
        "trainol = train.loc[(train['class'] ==1)]\n",
        "#print(trainol)\n",
        "\n",
        "testol = test.loc[(test['class'] ==1)]\n",
        "#print(testol\n",
        "\n",
        "#removendo os outliers para treinamento \n",
        "train= train[train['class'] != 1]\n",
        "\n",
        "y_train = []\n",
        "for i in range(59000):\n",
        "  y_train.append(True)\n",
        "\n",
        "train['class'].value_counts()\n",
        "test= test[test['class'] != 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmTXfBlA3J6o",
        "colab_type": "text"
      },
      "source": [
        "# Train data: mediana, m√©dia e mais frequente "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMK7c5Kr162x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using median\n",
        "#impute_median = SimpleImputer(missing_values= np.nan, strategy='median',copy = True, verbose= 2)\n",
        "#train_median = pd.DataFrame(impute_median.fit_transform(train),columns=train.columns)\n",
        "#train_median['class'].value_counts()\n",
        "#train_imputed_median.to_csv(\"Train_imputed_median\")\n",
        "\n",
        "#trainol_median = pd.DataFrame(impute_median.fit_transform(trainol),columns=trainol.columns)\n",
        "#trainol_median['class'].value_counts()\n",
        "\n",
        "# using mean\n",
        "impute_mean = SimpleImputer(missing_values= np.nan, strategy='mean',copy = True, verbose= 2)\n",
        "train_mean = pd.DataFrame(impute_mean.fit_transform(train),columns=train.columns)\n",
        "train_mean['class'].value_counts()\n",
        "#train_imputed_mean.to_csv(\"Train_imputed_mean\")\n",
        "\n",
        "trainol_mean = pd.DataFrame(impute_mean.fit_transform(trainol),columns=trainol.columns)\n",
        "trainol_mean['class'].value_counts()\n",
        "\n",
        "# using most frequent\n",
        "#impute_most_frequent = SimpleImputer(missing_values= np.nan, strategy='most_frequent',copy = True, verbose= 2)\n",
        "#train_most_frequent = pd.DataFrame(impute_most_frequent.fit_transform(train),columns=train.columns)\n",
        "#train_imputed_most_frequent.to_csv(\"Train_imputed_most_frequent\")\n",
        "\n",
        "#trainol_most_frequent = pd.DataFrame(impute_most_frequent.fit_transform(trainol),columns=trainol.columns)\n",
        "#trainol_most_frequent['class'].value_counts()\n",
        "\n",
        "# Feature engineering, creating a new feature set which indicates \n",
        "# the missing value in a given row and column as True else False\n",
        "#missing_impute = MissingIndicator()\n",
        "#miss = missing_impute.fit_transform(train)\n",
        "#train_miss_indi = pd.DataFrame(miss)\n",
        "#train_miss_indi.to_csv('train_miss_indi.csv')\n",
        "\n",
        "# ----------------------Test--------------------------------\n",
        "# using median\n",
        "#impute_median = SimpleImputer(missing_values= np.nan, strategy='median',copy = True, verbose= 2)\n",
        "#test_median = pd.DataFrame(impute_median.fit_transform(test),columns=test.columns)\n",
        "#test_median['class'].value_counts()\n",
        "#train_imputed_median.to_csv(\"Train_imputed_median\")\n",
        "\n",
        "# using mean\n",
        "#impute_mean = SimpleImputer(missing_values= np.nan, strategy='mean',copy = True, verbose= 2)\n",
        "test_mean = pd.DataFrame(impute_mean.fit_transform(test),columns=test.columns)\n",
        "#test_mean['class'].value_counts()\n",
        "#testol_mean = pd.DataFrame(impute_mean.fit_transform(testol),columns=testol.columns)\n",
        "#test_imputed_mean.to_csv(\"Test_imputed_mean\")\n",
        "\n",
        "# using most frequent\n",
        "#impute_most_frequent = SimpleImputer(missing_values= np.nan, strategy='most_frequent',copy = True, verbose= 2)\n",
        "#test_most_frequent = pd.DataFrame(impute_most_frequent.fit_transform(test),columns=test.columns)\n",
        "#test_imputed_most_frequent.to_csv(\"Test_imputed_most_frequent\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ZOAeqlLxk0",
        "colab_type": "text"
      },
      "source": [
        "# Padronizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx1LlW4VL08R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get column names first\n",
        "names = train_mean.columns\n",
        "\n",
        "# Create the Scaler object\n",
        "scaler = preprocessing.StandardScaler().fit(train_mean)\n",
        "\n",
        "# Fit your data on the scaler object - train_imputed_median\n",
        "#train_st = scaler.fit_transform(train_median)\n",
        "#train_st = pd.DataFrame(train_st, columns=names)\n",
        "#trainol_st = scaler.fit_transform(trainol_median)\n",
        "#trainol_st = pd.DataFrame(trainol_st, columns=names)\n",
        "\n",
        "# Fit your data on the scaler object - train_imputed_mean\n",
        "train_st = scaler.fit_transform(train_mean)\n",
        "train_st = pd.DataFrame(train_mean, columns=names)\n",
        "trainol_st = scaler.fit_transform(trainol_mean)\n",
        "trainol_st = pd.DataFrame(trainol_st, columns=names)\n",
        "\n",
        "# Fit your data on the scaler object - train_imputed_most_frequent\n",
        "#train_st = scaler.fit_transform(train_most_frequent)\n",
        "#train_st = pd.DataFrame(train_most_frequent, columns=names)\n",
        "#trainol_st = scaler.fit_transform(trainol_most_frequent)\n",
        "#trainol_st = pd.DataFrame(trainol_st, columns=names)\n",
        "\n",
        "# ----------------------Test--------------------------------\n",
        "\n",
        "# Fit your data on the scaler object - train_imputed_median\n",
        "#test_st = scaler.fit_transform(test_median)\n",
        "#test_st = pd.DataFrame(test_st, columns=names)\n",
        "\n",
        "test_st = scaler.fit_transform(test_mean)\n",
        "test_st = pd.DataFrame(test_st, columns=names)\n",
        "#testol_st = scaler.fit_transform(testol_mean)\n",
        "#testol_st = pd.DataFrame(testol_st, columns=names)\n",
        "\n",
        "#test_st = scaler.fit_transform(test_most_frequent)\n",
        "#test_st = pd.DataFrame(test_st, columns=names)\n",
        "#testol_st = scaler.fit_transform(testol_most_frequent)\n",
        "#testol_st = pd.DataFrame(testol_st, columns=names)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrHaO7S60TKk",
        "colab_type": "text"
      },
      "source": [
        "# Retirar a coluna ['class'] do train e trainol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTotZbjc0ZXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_st = train_st.drop(['class'], axis =1)\n",
        "trainol_st = trainol_st.drop(['class'], axis =1)\n",
        "test_st = test_st.drop(['class'], axis =1)\n",
        "#testol_st = testol_st.drop(['class'], axis =1)\n",
        "#print(test_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lEpfcbiVf8W",
        "colab_type": "text"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9_L6QUBVhe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pca_variance = 0.95\n",
        "pca = PCA(n_components=50)\n",
        "pca.fit(train_st)\n",
        "bestfeat_train_st = pca.transform(train_st)\n",
        "bestfeat_train_st = pd.DataFrame(bestfeat_train_st)\n",
        "#print(bestfeat_trainol_st)\n",
        "\n",
        "bestfeat_trainol_st = pca.transform(trainol_st)\n",
        "#print(bestfeat_trainol_st)\n",
        "\n",
        "bestfeat_test_st = pca.transform(test_st)\n",
        "#print(bestfeat_test_st)\n",
        "\n",
        "#print('Number of components: ', pca.n_components_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF0YWrbQ9U0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sum(pca.explained_variance_ratio_))\n",
        "#print(pca.components_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-jMFoNlR5H",
        "colab_type": "text"
      },
      "source": [
        "# Training: One-Class SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkiE7n7blRAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oneclass = svm.OneClassSVM()\n",
        "\n",
        "#oneclass.fit(train_st)\n",
        "#oneclass.fit(train_mean)\n",
        "#oneclass.fit(train_most_frequent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYB8TZ0eygZW",
        "colab_type": "text"
      },
      "source": [
        "# SearchGrid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB94QQ7kyg-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn import grid_search\n",
        "params = [{'gamma': [0.01], 'nu': [0.16, 0.1], 'kernel': ['rbf']}]\n",
        "newmodel = GridSearchCV(oneclass, params, scoring='precision', return_train_score=True, verbose=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQlCpQw3iP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6b7f20ec-9db5-4488-98a2-e7953715e7b9"
      },
      "source": [
        "newmodel.fit(train_st, y_train)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.16 .................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.16, score=(train=1.000, test=0.000), total=38.9min\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.16 .................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 45.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.16, score=(train=1.000, test=1.000), total=39.4min\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.16 .................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 91.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.16, score=(train=1.000, test=1.000), total=38.4min\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.1 ..................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 137.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.1, score=(train=1.000, test=0.000), total=40.7min\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.1 ..................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 184.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.1, score=(train=1.000, test=0.000), total=40.8min\n",
            "[CV] gamma=0.01, kernel=rbf, nu=0.1 ..................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 232.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  gamma=0.01, kernel=rbf, nu=0.1, score=(train=1.000, test=0.000), total=41.0min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 279.9min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 279.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=OneClassSVM(cache_size=200, coef0=0.0, degree=3,\n",
              "                                   gamma='auto_deprecated', kernel='rbf',\n",
              "                                   max_iter=-1, nu=0.5, random_state=None,\n",
              "                                   shrinking=True, tol=0.001, verbose=False),\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid=[{'gamma': [0.01], 'kernel': ['rbf'],\n",
              "                          'nu': [0.16, 0.1]}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
              "             scoring='precision', verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4BwfueEyfKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a8990023-c481-4676-82c7-9141a5c6dac7"
      },
      "source": [
        "print(newmodel.best_estimator_)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
            "            max_iter=-1, nu=0.16, random_state=None, shrinking=True, tol=0.001,\n",
            "            verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFvyqfFhAdTU",
        "colab_type": "text"
      },
      "source": [
        "# Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcdxGdIunzf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bounds = {'nu': (0.1, 1), 'gamma': (0.1, 1)}\n",
        "\n",
        "#optimizer = BayesianOptimization(f=oneclass, pbounds=bounds, random_state=1)\n",
        "#print(optimizer)\n",
        "#optimizer.maximize(init_points=2, n_iter=5)\n",
        "#print(optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8rJXPcSMUk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer.maximize()\n",
        "#print(optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6FC1exGDWU2",
        "colab_type": "text"
      },
      "source": [
        "# Salvar arquivo com Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P9g1gXLDV-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# salvar arquivo de treino\n",
        "filename = '/drive/My Drive/IFES/TCC_Scania/Redes treinadas/model_mean_sgcv1.sav'\n",
        "pickle.dump(newmodel.best_estimator_, open(filename, 'wb'))\n",
        "\n",
        "# carregar arquivo de treino\n",
        "#newmodel = pickle.load(open('/drive/My Drive/IFES/TCC_Scania/Redes treinadas/modelpca_median_sgcv3.sav', 'rb'))\n",
        "#result = loaded_model.score(X_test, Y_test)\n",
        "#print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDKGtt4tqRq1",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCok6LcqqTlQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fc6b5bff-b1cb-4a0f-f299-732391d568f6"
      },
      "source": [
        "#teste = trainol_median\n",
        "\n",
        "pred_train = newmodel.best_estimator_.predict(train_st)\n",
        "pred_outliers = newmodel.best_estimator_.predict(trainol_st)\n",
        "pred_test = newmodel.best_estimator_.predict(test_st)\n",
        "\n",
        "#print('pred teste = ', pred_teste)\n",
        "\n",
        "# tirado de: https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html\n",
        "n_error_train = pred_train[pred_train == -1].size\n",
        "n_error_outliers = pred_outliers[pred_outliers == 1].size\n",
        "n_error_test = pred_test[pred_test == -1].size\n",
        "print('erro train = ', n_error_train)\n",
        "print('erro outliers = ', n_error_outliers)\n",
        "print('erro teste = ', n_error_test)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "erro train =  8434\n",
            "erro outliers =  0\n",
            "erro teste =  15625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}